{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Keras + TensorFlow to train a Neural Network\n",
    "### to do Nonlinear Regression: remove bilinear (ASC) noise from DARM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "from __future__ import division\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.io import loadmat, savemat\n",
    "import scipy.signal as sig\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "from NonlinearRegression.tools.bilinearHelper import (cost_filter,\n",
    "                                                      downsample,\n",
    "                                                      get_cbc,\n",
    "                                                      load_data,\n",
    "                                                      normalize,\n",
    "                                                      plot_cost_asd,\n",
    "                                                      plot_results,\n",
    "                                                      plot_training_progress,\n",
    "                                                      set_plot_style,\n",
    "                                                      prepare_inputs)\n",
    "\n",
    "# Hush tensorflow warnings about AVX instructions\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "set_plot_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjustable Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datafile = '../../../MockData/DARM_with_bilinear.mat'\n",
    "# Training data\n",
    "val_frac = 1 / 4  # Amount of data to save for validation\n",
    "fs_slow  = 32     # Resample seismic data to this freq\n",
    "Tchunk   = 1 / 4  # Seconds of data used to predict each DARM sample\n",
    "Tbatch   = 1      # How many seconds of DARM per gradient update\n",
    "Nepoch   = 3      # Number of times to iterate over training data set\n",
    "# Neural Network\n",
    "Nlayers    = 9       # Number of fully connected layers\n",
    "cost_func  = 'mse'   # Mean Squared Error, i.e. PSD reduction\n",
    "optimizer  = 'adam'  # Seems to work well...\n",
    "activation = 'elu'   # \"Exponential linear unit\"\n",
    "dropout    = 0.05    # maybe this helps in training\n",
    "verbose    = 1       # Print training progress to stderr\n",
    "# Cost function\n",
    "zero_freqs = [6, 130]\n",
    "zero_order = [10, 2]\n",
    "pole_freqs = [12, 70]\n",
    "pole_order = [9, 3]\n",
    "# Output data and plots\n",
    "tfft      = 8\n",
    "save_data = False\n",
    "# Whether to look for previously saved downsampled data\n",
    "doLoadDownsampled = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if doLoadDownsampled:\n",
    "    print('Using previous downsampled data!')\n",
    "    # Load up downsampled datas\n",
    "    filename = 'DARM_with_bilinear_downsampled.mat'\n",
    "    datas    = loadmat(filename)\n",
    "\n",
    "    if datas['fs_slow'][0][0] != fs_slow:\n",
    "        raise ValueError(\"Decimated sampling rate from previously saved \"\n",
    "                         \" data (%.0f) different from requested (%.0f)\" %\n",
    "                         (datas['fs_slow'][0][0], fs_slow))\n",
    "\n",
    "    angular   = datas['angular']\n",
    "    beam_spot = datas['beam_spot']\n",
    "    Npairs    = angular.shape[0]\n",
    "\n",
    "    tar_raw = datas['tar_raw']\n",
    "    bg_raw  = datas['bg_raw']\n",
    "    tar     = datas['tar'][0]\n",
    "    bg      = datas['bg'][0]\n",
    "\n",
    "    scale       = datas['scale'][0][0]\n",
    "    invBP       = datas['invBP']\n",
    "    fs          = datas['fs'][0][0]\n",
    "    fs_slow     = datas['fs_slow'][0][0]\n",
    "    down_factor = int(fs // fs_slow)\n",
    "\n",
    "else:\n",
    "    # Load up datas\n",
    "    datafile = '../../../MockData/DARM_with_bilinear.mat'\n",
    "    bg_raw, tar_raw, wit, fs = load_data(datafile)\n",
    "\n",
    "    BP, invBP = cost_filter(fs, zero_freqs, pole_freqs, zero_order,\n",
    "                            pole_order)\n",
    "\n",
    "    print(\"Filtering and Decimating...\")\n",
    "    # remove mean and normalize to std for nicer NN learning\n",
    "    tar, scale = normalize(tar_raw, filter=BP)\n",
    "    bg, _      = normalize(bg_raw,  filter=BP, scale=scale)\n",
    "\n",
    "    # Get the witness signals ready for training.\n",
    "    Npairs = wit.shape[0] // 2  # How many ASC + beam spot pairs\n",
    "\n",
    "    # Shape the ASC control signal with the same filter as DARM\n",
    "    beam_spot, _ = normalize(wit[:Npairs])\n",
    "    angular, _   = normalize(wit[Npairs:], filter=BP)\n",
    "\n",
    "    # Since we only care about the slow beam spot motion, we\n",
    "    # don't need full rate information. Decimating the signal\n",
    "    # reduces the input vector length and the number of neurons\n",
    "    # we have to train.\n",
    "\n",
    "    down_factor = int(fs // fs_slow)\n",
    "    beam_spot = downsample(beam_spot, down_factor)\n",
    "\n",
    "    # save downsampled datas\n",
    "    downsampled_datafile = 'DARM_with_bilinear_downsampled.mat'\n",
    "    datas = {}\n",
    "    datas['angular'] = angular\n",
    "    datas['beam_spot'] = beam_spot\n",
    "    datas['fs'] = fs\n",
    "    datas['fs_slow'] = fs_slow\n",
    "    datas['tar_raw'] = tar_raw\n",
    "    datas['bg_raw'] = bg_raw\n",
    "    datas['tar'] = tar\n",
    "    datas['bg'] = bg\n",
    "    datas['scale'] = scale\n",
    "    datas['invBP'] = invBP\n",
    "\n",
    "    savemat(downsampled_datafile, datas, do_compression=True)\n",
    "        \n",
    "nfft = tfft * fs\n",
    "\n",
    "# How many DARM samples are saved for validation\n",
    "Nval = int(tar_raw.size * val_frac)\n",
    "\n",
    "# How many witness samples are used to predict each DARM sample\n",
    "Nchunk = int(Tchunk * fs)\n",
    "Nbatch = int(Tbatch * fs)\n",
    "\n",
    "Nang = Nchunk\n",
    "Nspot = Nchunk // down_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cost function as an ASD\n",
    "plot_cost_asd(tar, bg, fs, nfft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Select training and validation data segments\n",
    "training_target = tar[:-Nval]\n",
    "training_spt    = beam_spot[:, :-Nval // down_factor]\n",
    "training_ang    = angular[:, :-Nval]\n",
    "\n",
    "validation_target = tar[-Nval:]\n",
    "validation_bg     =  bg[-Nval:]\n",
    "validation_spt    = beam_spot[:, -Nval // down_factor:]\n",
    "validation_ang    = angular[:, -Nval:]\n",
    "\n",
    "# Create stacked, strided input arrays\n",
    "training_input = prepare_inputs(training_spt, training_ang, Nchunk)\n",
    "validation_input = prepare_inputs(validation_spt, validation_ang, Nchunk)\n",
    "\n",
    "# Minimum loss is achieved when target - prediction = bg\n",
    "# Thus, MSE = mean(bg**2), i.e. var(bg)\n",
    "minLoss = np.var(bg)\n",
    "\n",
    "# Rescale validation data back to DARM units\n",
    "validation_darm = validation_target * scale\n",
    "validation_bg  *= scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct the neural network and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the network topology  -- -- -- - - -  -  -   -   -    -\n",
    "input_shape = (Npairs * (Nang + Nspot),)\n",
    "model = Sequential()\n",
    "model.add(Dense(input_shape[0], input_shape=input_shape,\n",
    "                activation='linear'))\n",
    "\n",
    "# this layer increases training time but not increase performance\n",
    "model.add(Dense(input_shape[0], activation=activation))\n",
    "model.add(Dropout(dropout))\n",
    "\n",
    "# add layers; decrease size of each by half\n",
    "layer_sizes = range(1, Nlayers)\n",
    "layer_sizes.reverse()\n",
    "for k in layer_sizes:\n",
    "    model.add(Dense(2**k, activation=activation))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "model.add(Dense(1, activation='linear'))\n",
    "model.compile(optimizer=optimizer, loss=cost_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting Network learning...\")\n",
    "print('Best achievable cost: {:.5g}'.format(minLoss))\n",
    "t_start = timer()\n",
    "\n",
    "roomba = model.fit(training_input, training_target,\n",
    "                   validation_data = (validation_input, validation_target),\n",
    "                   batch_size      = Nbatch,\n",
    "                   epochs          = Nepoch,\n",
    "                   verbose         = verbose)\n",
    "\n",
    "print(str(round(timer() - t_start)) + \" seconds for Training.\")\n",
    "plot_training_progress(roomba, minLoss=minLoss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Generate final validation prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Applying model to input data...\n",
    "validation_out = model.predict(validation_input, batch_size=Nbatch)[:, 0]\n",
    "validation_out *= scale  # Scale to DARM units\n",
    "\n",
    "# Unwhitening target and output data...\")\n",
    "validation_darm = sig.sosfilt(invBP, validation_darm)\n",
    "validation_bg   = sig.sosfilt(invBP, validation_bg)\n",
    "validation_out  = sig.sosfilt(invBP, validation_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot results, print model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(validation_darm, validation_out, validation_bg, fs, nfft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Saving model and processing params...\n",
    "if save_data:\n",
    "    model.save('FF_RegressionModel.h5')\n",
    "    output_data = {\n",
    "        'history': roomba.history,\n",
    "        'invBP': invBP,\n",
    "        'scale': scale,\n",
    "        'fs': fs,\n",
    "        'nfft': nfft,\n",
    "    }\n",
    "    savemat('Results_TFregression.mat', output_data, do_compression=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
